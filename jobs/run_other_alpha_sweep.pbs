#!/bin/bash
#PBS -N vectors_exp
#PBS -q GPU-1
#PBS -o log/alpha_sweep.o%j
#PBS -e log/alpha_sweep.e%j
#PBS -l select=1:ncpus=8:ngpus=1:mem=64gb
#PBS -l walltime=20:00:00
#PBS -j oe

set -euo pipefail

WORKDIR="${PBS_O_WORKDIR:-$PWD}"
RUN_ID="${PBS_JOBID:-bash_$(date +%Y%m%d_%H%M%S)}"

cd "$WORKDIR"
mkdir -p log
LOG_FILE="log/alpha_sweep.${RUN_ID}.log"
exec > >(tee -a "$LOG_FILE") 2>&1

echo "=== STARTING EFFICIENT FULL EXPERIMENT PIPELINE ==="
echo "START TIME: $(date)"
echo "Host: $(hostname)"
echo "CWD : $PWD"
echo "PBS_JOBID: ${PBS_JOBID:-}"
echo "RUN_ID: $RUN_ID"
echo "WORKDIR: $WORKDIR"
echo "LOG_FILE: $LOG_FILE"

# ==================== Project ====================
export PROJECT_DIR="$WORKDIR"
export PYTHONPATH="$PROJECT_DIR/src:$PROJECT_DIR:$PROJECT_DIR/scripts:${PYTHONPATH:-}"

# キャッシュ設定
export HF_HOME="$PROJECT_DIR/.hf_cache"
export TRANSFORMERS_CACHE="$HF_HOME"
export HF_DATASETS_CACHE="$HF_HOME/datasets"
export HUGGINGFACE_HUB_CACHE="$HF_HOME/hub"

export OMP_NUM_THREADS=1
export HF_HUB_ENABLE_HF_TRANSFER=0
export PYTORCH_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false

echo "[INFO] nvidia-smi:"
command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi || echo "(no nvidia-smi)"

# ==================== Tokens ====================
set +x
if [ -f "$PROJECT_DIR/.hf_token" ]; then
  export HUGGINGFACE_HUB_TOKEN="$(head -n1 "$PROJECT_DIR/.hf_token" | tr -d '\r\n' | sed 's/^Bearer[[:space:]]\+//')"
fi

# ==================== Config backup & restore ====================
CONFIG_FILE="exp/configs/big5_vectors.yaml"
CONFIG_BAK="exp/configs/big5_vectors.yaml.bak"

restore_config() {
  set +e
  if [ -f "$CONFIG_BAK" ]; then
    echo "[CLEANUP] Restoring $CONFIG_FILE from $CONFIG_BAK..."
    cp "$CONFIG_BAK" "$CONFIG_FILE"
    rm -f "$CONFIG_BAK"
  fi
}
trap restore_config EXIT

if [ ! -f "$CONFIG_BAK" ]; then
  echo "[Step 0] Backing up $CONFIG_FILE to $CONFIG_BAK..."
  cp "$CONFIG_FILE" "$CONFIG_BAK"
else
  echo "[Step 0] Backup exists; restoring from it."
  cp "$CONFIG_BAK" "$CONFIG_FILE"
fi

# ==================== uv venv ====================
VENV="$PROJECT_DIR/persona_steering"
if [ -x "$VENV/bin/python" ]; then
  # UV environment exists
  export UV_CACHE_DIR="${UV_CACHE_DIR:-$PROJECT_DIR/.uv_cache}"
  export VIRTUAL_ENV="$VENV"
  export PATH="$VENV/bin:$PATH"
  export PYTHON_BIN="$VENV/bin/python"

  if [ "${RUN_UV_SYNC:-0}" = "1" ]; then
    if [ -f "$PROJECT_DIR/uv.lock" ]; then
      uv sync --frozen --active
    else
      uv sync --active
    fi
  fi
else
  CONDA_PYTHON="/home/s2550009/anaconda3/envs/persona_vectors/bin/python"
  if [ -x "$CONDA_PYTHON" ]; then
    echo "[WARN] venv not found at $VENV. Using conda env: $CONDA_PYTHON"
    export PYTHON_BIN="$CONDA_PYTHON"
    # Assuming conda env handles library paths reasonably well, but we still might want to be careful.
    # We are NOT activating it fully (no source activate), just using the binary.
  else
    # Fallback to system python (e.g. for ksgayai)
    echo "[WARN] venv not found at $VENV. Falling back to system python."
    if ! command -v python >/dev/null 2>&1; then
       echo "[ERROR] python not found in PATH."
       exit 1
    fi
    export PYTHON_BIN="$(command -v python)"
    echo "Using PYTHON_BIN=$PYTHON_BIN"
  fi
fi

# --- Make pip-installed NVIDIA libs visible ---
export LD_LIBRARY_PATH="$($PYTHON_BIN - <<'PY'
import site, glob, os
paths=[]
for sp in site.getsitepackages():
    paths += glob.glob(os.path.join(sp, "nvidia", "*", "lib"))
seen=set(); out=[]
for p in paths:
    if p not in seen:
        out.append(p); seen.add(p)
print(":".join(out))
PY
):${LD_LIBRARY_PATH:-}"

"$PYTHON_BIN" -V
"$PYTHON_BIN" - <<'PY'
import torch
print("torch", torch.__version__)
print("cuda available", torch.cuda.is_available())
if torch.cuda.is_available():
    print("gpu", torch.cuda.get_device_name(0))
PY

# ==================== Experiment params ====================
TRAITS=("openness" "conscientiousness" "extraversion" "agreeableness" "neuroticism")

MODEL_SPECS=(
  "mistral_7b|mistralai/Mistral-7B-v0.3|mistralai/Mistral-7B-Instruct-v0.3|-5,-4,-3,-2,-1,0,1,2,3,4,5|-5,-4,-3,-2,-1,0,1,2,3,4,5"
  "llama3_8b|meta-llama/Meta-Llama-3-8B|meta-llama/Meta-Llama-3-8B-Instruct|-20,-16,-12,-8,-4,0,4,8,12,16,20|-20,-16,-12,-8,-4,0,4,8,12,16,20"
  "olmo3_7b|allenai/Olmo-3-1025-7B|allenai/Olmo-3-7B-Instruct|-50,-40,-30,-20,-10,0,10,20,30,40,50|-50,-40,-30,-20,-10,0,10,20,30,40,50"
  "qwen25_7b|Qwen/Qwen2.5-7B|Qwen/Qwen2.5-7B-Instruct|-160,-140,-120,-100,-80,0,80,100,120,140,160|-160,-140,-120,-100,-80,0,80,100,120,140,160"
  "gemma2_9b|google/gemma-2-9b|google/gemma-2-9b-it|-500,-400,-300,-200,-100,0,100,200,300,400,500|-500,-400,-300,-200,-100,0,100,200,300,400,500"
  "falcon3_7b|tiiuae/Falcon3-7B-Base|tiiuae/Falcon3-7B-Instruct|-500,-400,-300,-200,-100,0,100,200,300,400,500|-500,-400,-300,-200,-100,0,100,200,300,400,500"
)

# ---------- helpers ----------
is_nonempty_file() { local f="$1"; [[ -f "$f" && -s "$f" ]]; }

prepare_axes_if_needed() {
  local model_id="$1"
  local ax_bank="$2"
  local config_file="$3"
  local norm_bank="${ax_bank%.npz}_rawnorms.npz"

  sed -i "s|^model_name: .*|model_name: ${model_id}|g" "$config_file"

  if is_nonempty_file "$ax_bank" && is_nonempty_file "$norm_bank"; then
    echo "[SKIP] axes+rawnorms exists: $ax_bank , $norm_bank"
  else
    echo "[RUN ] 00_prepare_vectors.py -> $ax_bank (+ $norm_bank)"
    "$PYTHON_BIN" scripts/00_prepare_vectors.py --config "$config_file" --bank_path "$ax_bank"
  fi
}

run_probe_if_needed() {
  local split="$1"
  local trait="$2"
  local model_id="$3"
  local axes_bank="$4"
  local out_jsonl="$5"
  local alpha_list="$6"

  if is_nonempty_file "$out_jsonl"; then
    echo "[SKIP] probe exists: $out_jsonl"
    return 0
  fi

  echo "[RUN ] 01_run_probe.py -> $out_jsonl (split=$split trait=$trait)"
  "$PYTHON_BIN" scripts/01_run_probe.py \
    --model       "$model_id" \
    --axes_bank   "$axes_bank" \
    --trait       "$trait" \
    --alpha_list="$alpha_list" \
    --out         "$out_jsonl" \
    --prompt_file "exp/01_probe_inputs/all_traits_unseen_100.json"
}

concat_alltraits() {
  local results_dir="$1"
  local split="$2"
  local out_all="$3"

  rm -f "$out_all"
  for trait in "${TRAITS[@]}"; do
    local f="${results_dir}/${tag}_${split}_${trait}_probe_results.jsonl"
    if is_nonempty_file "$f"; then
      cat "$f" >> "$out_all"
    else
      echo "[WARN] missing/empty: $f"
    fi
  done
  echo "[INFO] wrote: $out_all"
}

# ★★★ Improved Text Analysis Helper ★★★
run_text_analysis() {
  local tag="$1"
  local results_dir="$2"
  local split="$3"      # base | instruct
  
  local input_jsonl="${results_dir}/${tag}_${split}_alltraits.jsonl"
  
  if ! is_nonempty_file "$input_jsonl"; then
    echo "[WARN] Text analysis skipped: $input_jsonl not found or empty"
    return 0
  fi

  # --- 13: Edit Distance (CPU) ---
  local dist_csv="${results_dir}/${tag}_${split}_edit_distance.csv"
  
  # Check if exists and not empty
  if is_nonempty_file "$dist_csv"; then
    echo "[SKIP] 13_text_change_vs_alpha.py exists: $dist_csv"
  else
    echo "[RUN ] 13_text_change_vs_alpha.py -> $dist_csv"
    "$PYTHON_BIN" scripts/13_text_change_vs_alpha.py "$input_jsonl" \
      --output "$dist_csv" \
      --baseline 0
  fi

  # --- 14: Personality Score (GPU) ---
  local score_csv="${results_dir}/${tag}_${split}_personality_scores.csv"
  
  # Check if exists and not empty
  if is_nonempty_file "$score_csv"; then
    echo "[SKIP] 14_calc_personality_score.py exists: $score_csv"
  else
    echo "[RUN ] 14_calc_personality_score.py -> $score_csv"
    "$PYTHON_BIN" scripts/14_calc_personality_score.py "$input_jsonl" \
      --output "$score_csv" \
      --batch_size 32 \
      --model "Minej/bert-base-personality"
  fi
}

run_alpha_select_and_viz() {
  local tag="$1"
  local results_dir="$2"
  local sel_rng_root="$3"

  mkdir -p "$sel_rng_root"

  for SPLIT in base instruct; do
    for trait in "${TRAITS[@]}"; do
      local in_jsonl="${results_dir}/${tag}_${SPLIT}_${trait}_probe_results.jsonl"
      [ -s "$in_jsonl" ] || continue

      "$PYTHON_BIN" scripts/06_alpha_eval_v13.py \
        --in "$in_jsonl" \
        --out_root "$sel_rng_root" \
        --per_prompt \
        --pass_rate_min 0.8 \
        --sem_min 0.0 \
        --len_ratio_min 0.5 \
        --len_ratio_max 2.0 \
        --distinct2_min 0.3 \
        --punct_ratio_max 0.85 \
        --min_phrase_tokens 3 \
        --max_run_token_max 8 \
        --max_run_phrase_max 2
    done
  done

  mkdir -p "$sel_rng_root/_summary"
  "$PYTHON_BIN" scripts/07_alpha_visualize.py \
    --globs "$sel_rng_root/range/*_per_prompt.jsonl" \
    --out_csv "$sel_rng_root/_summary/per_prompt_summary.csv" \
    --out_dir "$sel_rng_root/_summary/per_prompt_figs"


}

run_slopes_and_viz() {
  local tag="$1"
  local results_dir="$2"
  local base_all="${results_dir}/${tag}_base_alltraits.jsonl"
  local instr_all="${results_dir}/${tag}_instruct_alltraits.jsonl"

  if ! is_nonempty_file "$base_all" || ! is_nonempty_file "$instr_all"; then
    echo "[WARN] missing alltraits jsonl for slopes (skip)"
    return 0
  fi

  local slopes_dir="${results_dir}/slopes"
  local figs_dir="${slopes_dir}/figs"
  mkdir -p "$slopes_dir" "$figs_dir"
  local out_csv="${slopes_dir}/slopes_${tag}_asst_pairwise.csv"

  if is_nonempty_file "$out_csv"; then
    echo "[SKIP] slopes csv exists: $out_csv"
  else
    echo "[RUN ] 02_probe_slopes_from_logs.py -> $out_csv"
    "$PYTHON_BIN" scripts/02_probe_slopes_from_logs.py \
      --base_json  "$base_all" \
      --instr_json "$instr_all" \
      --out_csv    "$out_csv" \
      --pooling asst \
      --axis_mode pairwise
  fi

  echo "[RUN ] 03_slopes_visualize.py"
  "$PYTHON_BIN" scripts/03_slopes_visualize.py --input_dir "$out_csv" --out_dir "$figs_dir" --value_col slope_delta_score_vs_alpha01
  "$PYTHON_BIN" scripts/03_slopes_visualize.py --input_dir "$out_csv" --out_dir "$figs_dir" --value_col slope_delta_score_vs_alpha
}

run_one_model_pair() {
  local tag="$1"
  local base_id="$2"
  local instr_id="$3"
  local alphas_base="$4"
  local alphas_instr="$5"

  echo "==== ${tag} / pooling=asst / mode=pairwise ===="

  local results_dir="exp/${tag}/asst_pairwise_results"
  mkdir -p "$results_dir"

  # --- Base axes ---
  echo "[${tag}] Base axes"
  local ax_base="exp/${tag}/axes_base_asst_pairwise.npz"
  prepare_axes_if_needed "$base_id" "$ax_base" "$CONFIG_FILE"

  for trait in "${TRAITS[@]}"; do
    run_probe_if_needed "base" "$trait" "$base_id" "$ax_base" \
      "${results_dir}/${tag}_base_${trait}_probe_results.jsonl" "$alphas_base"
  done
  concat_alltraits "$results_dir" "base" "${results_dir}/${tag}_base_alltraits.jsonl"
  
  # Base Text Analysis
  run_text_analysis "$tag" "$results_dir" "base"

  # --- Instruct axes ---
  echo "[${tag}] Instruct axes"
  local ax_instr="exp/${tag}/axes_instruct_asst_pairwise.npz"
  prepare_axes_if_needed "$instr_id" "$ax_instr" "$CONFIG_FILE"

  for trait in "${TRAITS[@]}"; do
    run_probe_if_needed "instruct" "$trait" "$instr_id" "$ax_instr" \
      "${results_dir}/${tag}_instruct_${trait}_probe_results.jsonl" "$alphas_instr"
  done
  concat_alltraits "$results_dir" "instruct" "${results_dir}/${tag}_instruct_alltraits.jsonl"

  # Instruct Text Analysis
  run_text_analysis "$tag" "$results_dir" "instruct"

  # --- 02/03 Slopes ---
  run_slopes_and_viz "$tag" "$results_dir"

  # --- 06/07 Alpha Select ---
  local sel_rng_root="${results_dir}/selected_range"
  run_alpha_select_and_viz "$tag" "$results_dir" "$sel_rng_root"

  echo "==== ${tag} END TIME: $(date)===="
  echo "-----------------------------------------"
}

# ==================== Run all ====================
for spec in "${MODEL_SPECS[@]}"; do
  IFS='|' read -r TAG BASE_ID INSTR_ID ALPHAS_BASE ALPHAS_INSTR <<< "$spec"
  run_one_model_pair "$TAG" "$BASE_ID" "$INSTR_ID" "$ALPHAS_BASE" "$ALPHAS_INSTR"
done

echo "==== GLOBAL CORR (all models) ===="

# 1) Join range summaries
"$PYTHON_BIN" - <<'PY'
import glob, os
import pandas as pd

paths = sorted(glob.glob("exp/*/asst_pairwise_results/selected_range/_summary/range_summary.csv"))
if paths:
    dfs=[]
    for p in paths:
        try:
            df=pd.read_csv(p)
            if "kind" in df.columns:
                df=df[df["kind"].astype(str).str.lower()=="range"].copy()
            dfs.append(df)
        except Exception as e:
            print(f"Skipping {p}: {e}")
    
    if dfs:
        out=pd.concat(dfs, ignore_index=True)
        os.makedirs("exp/_all/asst_pairwise_results/selected_range/_summary", exist_ok=True)
        out.to_csv("exp/_all/asst_pairwise_results/selected_range/_summary/range_summary.csv", index=False)
        print("Merged range summaries.")
else:
    print("No range summaries found.")
PY



echo "=== PIPELINE COMPLETED ==="

echo "=== VISUALIZING TEXT METRICS FOR ALL MODELS ==="

for spec in "${MODEL_SPECS[@]}"; do
  IFS='|' read -r TAG BASE_ID INSTR_ID ALPHAS_BASE ALPHAS_INSTR <<< "$spec"
  
  echo "[Viz] Generating radar charts for: $TAG"
  OUT_DIR="exp/${TAG}/asst_pairwise_results/plots"
  mkdir -p "$OUT_DIR"

  "$PYTHON_BIN" scripts/15_text_sensitivity_visualize.py \
    --dist_glob "exp/${TAG}/asst_pairwise_results/*_edit_distance.csv" \
    --score_glob "exp/${TAG}/asst_pairwise_results/*_personality_scores.csv" \
    --out_dir "$OUT_DIR" \
    --tag "$TAG"

  "$PYTHON_BIN" scripts/16_visualize_combined_metrics.py \
  --internal_csv "exp/${TAG}/asst_pairwise_results/slopes/slopes_${TAG}_asst_pairwise.csv" \
  --external_csv "exp/${TAG}/asst_pairwise_results/plots/${TAG}_text_sensitivities.csv" \
  --out_dir "exp/${TAG}/asst_pairwise_results/plots" \
  --tag "${TAG}"
    
  echo " -> Saved to $OUT_DIR"
done

echo "=== CROSS MODEL COMPARISON ==="
"$PYTHON_BIN" scripts/17_cross_model_comparison.py \
  --root_dir "exp" \
  --out_dir "exp/_all/comparison_plots"

echo "=== SCATTER PLOT VISUALIZATION ==="
"$PYTHON_BIN" scripts/18_visualize_scatter.py --root_dir "exp" --suffix ""